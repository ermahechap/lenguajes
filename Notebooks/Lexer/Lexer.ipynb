{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexer\n",
    "**Made by:** Juan David Barragan, Jose Daniel Organista, Edwin Ricardo Mahecha\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load assets\n",
    "Here we load the tokens and reserved words from a file in order to generate the corresponding regular expresions(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file imports\n",
    "path_tokens = 'tokens.txt'\n",
    "path_reserved = 'reserved.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserved set\n",
    "f = open(path_reserved)\n",
    "reserved_set = set([x.strip() for x in f.readlines()])\n",
    "f.close()\n",
    "\n",
    "#Generate all regex we will need\n",
    "\n",
    "# Util\n",
    "regex = {\n",
    "    'id' : '[_a-zA-Z_]*[A-Za-z0-9]+[_A-Za-z0-9_]*',\n",
    "    'string': r'([\"])(?:(?=(\\\\?))\\2.)*?\\1',\n",
    "    'numerico' : '[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-][0-9]+)?',\n",
    "    'comment' : '#'\n",
    "}\n",
    "# Token Regex - Awuful\n",
    "f = open(path_tokens,'r')\n",
    "token_array = [x.strip().split('\\t') for x in f.readlines()]\n",
    "tokens = [x[0] for x in  token_array]\n",
    "f.close()\n",
    "\n",
    "token_set = {k:v for k,v in token_array}\n",
    "\n",
    "token_regex = r''.join('(?:'+re.escape(y)+')|' for y in [x[0] for x in  token_array])[:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions\n",
    "We create these functions in order to identify up to which point does a regex match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util\n",
    "\n",
    "def whileFullMatch(reg, line, c, start = 0): #returns last column from line reference and full matched string\n",
    "    col = start+1\n",
    "    #print(len(line))\n",
    "    while(col < len(line) and re.fullmatch(reg,c)):\n",
    "        c += line[col]\n",
    "        col += 1\n",
    "    \n",
    "    if(re.fullmatch(reg, c)):\n",
    "        return col, c\n",
    "    return col-1, c[:-1]\n",
    "    '''\n",
    "    for p in line[col:]:\n",
    "        c+=p\n",
    "        if not re.fullmatch(reg,p):\n",
    "            print(c)\n",
    "            c = c[:-1]\n",
    "            break\n",
    "        col+=1\n",
    "    '''\n",
    "    \n",
    "    return col, c\n",
    "\n",
    "def untilFullMatch(reg, line, c, start = 0): #return las column fro line reference and full matched string\n",
    "    col = start+1\n",
    "    #print(line)\n",
    "    #print(len(line))\n",
    "    while(col < len(line) and not re.fullmatch(reg,c)):\n",
    "        c += line[col]\n",
    "        col += 1\n",
    "    \n",
    "    if(re.fullmatch(reg, c)):\n",
    "        return col, c\n",
    "    return col-1, c[:-1]\n",
    "\n",
    "def getMore(line, col, n=1): #gets the next n characters from line starting in com\n",
    "    if col+n > len(line):\n",
    "        return len(line),line[col:]\n",
    "    return col+n, line[col:col+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 >>\n",
      "3 :=:\n"
     ]
    }
   ],
   "source": [
    "c='>'\n",
    "line='>>:=:'\n",
    "col, match = whileFullMatch(token_regex, line, c)\n",
    "col_, match_ = whileFullMatch(token_regex, line[col:], line[col])\n",
    "\n",
    "print(col,match)\n",
    "print(col_, match_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "### Tokens\n",
    "Here we define a class to parse the tokens properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self,row,col,_type = None, lexeme = None):\n",
    "        self.row = row + 1\n",
    "        self.col = col + 1\n",
    "        self.token_type = _type\n",
    "        if lexeme:\n",
    "            self.lexeme = lexeme\n",
    "        else:\n",
    "            self.lexeme = _type\n",
    "        self.error = False\n",
    "        \n",
    "    def parse(self):\n",
    "        if not self.token_type and not self.lexeme:\n",
    "            return '>>> Error lexico(linea:{},posicion:{})'.format(self.row,self.col)\n",
    "        if self.lexeme == self.token_type:\n",
    "            return '<{},{},{}>'.format(self.token_type, self.row, self.col)\n",
    "        return '<{},{},{},{}>'.format(self.token_type, self.lexeme, self.row, self.col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "This is class reads a file an processes. Each call of *nextToken()* returns the next token found in the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.flag = False #Flag for numeric special case\n",
    "        self.error = False\n",
    "        \n",
    "    def readFile(self):\n",
    "        f = open(self.filepath,'r')\n",
    "        self.lines = f.readlines()\n",
    "        f.close()\n",
    "        #reset\n",
    "        self.displacement = self.row = self.col = 0\n",
    "        self.flag = self.error = False\n",
    "    \n",
    "    def cleanLine(self, line):\n",
    "        l = 0\n",
    "        for c in line:\n",
    "            if(c==' '):\n",
    "                self.col+=1\n",
    "            elif(c=='\\t'):\n",
    "                self.col += 1\n",
    "                self.displacement += 3\n",
    "            else:\n",
    "                break\n",
    "            l+=1\n",
    "        return line[l:].strip()\n",
    "    \n",
    "    def number(self, c, line):\n",
    "        # the col we use here is local\n",
    "        # the self.col currently \n",
    "        global regex\n",
    "        reg = regex['numerico']\n",
    "        col, match = whileFullMatch(reg, line, c)\n",
    "\n",
    "        t_col, two_more = getMore(line, col, n = 2)\n",
    "        if re.match(r'[\\.][0-9]', two_more):\n",
    "            match += two_more\n",
    "            col = t_col\n",
    "            col, match = whileFullMatch(reg, line, match, col - 1) # col - 1 because function adds one\n",
    "        \n",
    "        t_col, three_more = getMore(line, col, n = 3)\n",
    "        if re.match(r'[eE][\\+-][0-9]', three_more):\n",
    "            match += three_more\n",
    "            col = t_col\n",
    "            col, match = whileFullMatch(reg, line, match, col - 1) # col - 1 because function adds one\n",
    "        tk = Token(self.row, self.col + self.displacement,'tk_num', match)\n",
    "        self.col += col #increment for the next read\n",
    "        \n",
    "        self.flag = False # Numeric special case\n",
    "        return tk\n",
    "    \n",
    "    def identifier(self, c, line):\n",
    "        global regex, reserved_set,token_regex, token_set\n",
    "        reg = regex['id']\n",
    "        col, match = whileFullMatch(reg, line, c)\n",
    "        #print(col,match)\n",
    "        if match in reserved_set:\n",
    "            tk = Token(self.row, self.col + self.displacement, match)\n",
    "            \n",
    "            #print(tk.token_type, tk.lexeme)\n",
    "            self.col += col #increment for the next read\n",
    "            \n",
    "            self.flag = False # Numeric special case\n",
    "            return tk\n",
    "        \n",
    "        tk = None\n",
    "        if(re.fullmatch(token_regex,match)):\n",
    "            tk = Token(self.row, self.col + self.displacement,token_set[match])\n",
    "        else:\n",
    "            tk = Token(self.row, self.col + self.displacement,'id', match)\n",
    "        \n",
    "        \n",
    "        self.col += col #increment for the next read\n",
    "        \n",
    "        self.flag = True # numeric special case\n",
    "        return tk\n",
    "    \n",
    "    def tokenMatch(self, c, line):\n",
    "        global token_regex, token_set\n",
    "        \n",
    "        if(not re.match(token_regex, c)):\n",
    "            tk = Token(self.row, self.col + self.displacement) #Error Token\n",
    "            self.error = True\n",
    "            return tk\n",
    "        \n",
    "        \n",
    "        col, match = whileFullMatch(token_regex, line, c)\n",
    "        \n",
    "        # try to match another token. Useful for tokens that are\n",
    "        # concatenations of tokens. ex: -:=, put inside while if the\n",
    "        \n",
    "        if(col+1<len(line)):\n",
    "            col_, match_ = whileFullMatch(token_regex, line[col:], line[col])\n",
    "            if(match_==':=:'):\n",
    "                match_=':='\n",
    "                col_-=1;\n",
    "            if(re.fullmatch(token_regex,match+match_)):\n",
    "                col+=col_\n",
    "                match += match_\n",
    "        \n",
    "        tk = Token(self.row, self.col + self.displacement, token_set[match])\n",
    "        self.col += col #increment for the next read\n",
    "        \n",
    "        self.flag = False # Numeric special case\n",
    "        return tk\n",
    "\n",
    "    def default(self, c, line):\n",
    "        if(re.match(r'[0-9]', c)):\n",
    "            #print('>>>NUM')\n",
    "            return self.number(c, line)\n",
    "        elif(re.match(r'[a-zA-Z_]', c)):\n",
    "            #print('>>>ID')\n",
    "            return self.identifier(c,line)\n",
    "        else:\n",
    "            #print('>>>TOKEN_MATCH')\n",
    "            return self.tokenMatch(c,line)\n",
    "    \n",
    "    def sign(self, c, line):\n",
    "        t_col, one_more = getMore(line, 1)\n",
    "        if re.match(r'[0-9]', one_more):\n",
    "            if not self.flag:\n",
    "                # case, include minus sign within number\n",
    "                self.col +=1\n",
    "                tk = self.number(c+one_more, line[1:])\n",
    "                tk.col-=1\n",
    "                return tk\n",
    "        \n",
    "        self.flag = False # Numeric special case\n",
    "        return self.tokenMatch(c, line)\n",
    "    \n",
    "    def stri(self, c, line): #strings\n",
    "        global regex\n",
    "        reg = regex['string']\n",
    "        col, match = untilFullMatch(reg, line, c)\n",
    "        if (re.fullmatch(reg, match)):\n",
    "            tk = Token(self.row, self.col + self.displacement, 'tk_cadena', match)\n",
    "            self.col += col\n",
    "            self.flag = False # Numeric special case\n",
    "            return tk\n",
    "        \n",
    "        tk = Token(self.row, self.col + self.displacement) #Error Token\n",
    "        self.error = True\n",
    "        self.flag = False # Numeric special case\n",
    "        return tk\n",
    "    \n",
    "    def dot(self, c, line):\n",
    "        t_col, two_more = getMore(line, 1, n=2)\n",
    "        if c+two_more == '...':\n",
    "            tk = Token(self.row, self.col + self.displacement,'tk_slice')\n",
    "            self.col+=3\n",
    "            self.flag = False # Numeric special case\n",
    "            return tk\n",
    "        self.flag = False # Numeric special case\n",
    "        return self.default(c,line)\n",
    "    \n",
    "    def nextToken(self):\n",
    "        if self.error: return ''\n",
    "        flag = True\n",
    "        line = ''\n",
    "        while(line == ''):\n",
    "            if self.row >= len(self.lines): return ''\n",
    "            line = self.lines[self.row][self.col:]\n",
    "            line = self.cleanLine(line)\n",
    "            if line == '' or line[0] == '#': # commentaries handling\n",
    "                self.row += 1\n",
    "                self.col = 0\n",
    "                self.displacement = 0\n",
    "                line = ''\n",
    "        #Process\n",
    "        c = line[0]\n",
    "        opt = {\n",
    "            '+' : self.sign,\n",
    "            '-' : self.sign,\n",
    "            '\"' : self.stri,\n",
    "            '.' : self.dot\n",
    "        }\n",
    "        f = opt.get(c, self.default)\n",
    "        #print('-----------')\n",
    "        #print(line)\n",
    "        #print(\">>\",c)\n",
    "        token = f(c, line)\n",
    "        #print(self.row+1, self.col+1)\n",
    "        return token.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we use the Lexer class to declare an object in order to start reading the tokens. We implement a while loop to keep reading the next token until EOF or an error is thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = 'in.txt'\n",
    "out_path = 'out.txt'\n",
    "lexer = Lexer(in_file)\n",
    "lexer.readFile()\n",
    "\n",
    "f = open(out_path,'w')\n",
    "while (True):\n",
    "    s = lexer.nextToken()\n",
    "    if s=='':\n",
    "        break\n",
    "    #print(s)\n",
    "    f.write(s+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to compare output and the expected output(only linux users I guess!). It should not show any message unless there is a mismatch between files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: expected_out.txt: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "#Note: Both files must have been created on linux or windows, otherwise it will show diffs\n",
    "!diff out.txt expected_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others\n",
    "This last part of code is not related to the lexer, we use it in order to extract the tokens and identifiers found on the examples' outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tokens and ids\n",
    "paths = ['out_1.txt', 'out_2.txt', 'out_3.txt']\n",
    "tks = set()\n",
    "ids = set()\n",
    "for p in paths:\n",
    "    p = 'Tests/'+p\n",
    "    f = open(p,'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for l in lines:\n",
    "        val = re.search('<(.*?),',l).span()\n",
    "        lex = l[val[0]+1:val[1]-1]\n",
    "        if lex[0:2] == 'tk':\n",
    "            tks.add(lex)\n",
    "        else:\n",
    "            ids.add(lex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tk_cor_der\n",
      "tk_ejecuta\n",
      "tk_cadena\n",
      "tk_multi\n",
      "tk_cor_izq\n",
      "tk_distinto\n",
      "tk_par_der\n",
      "tk_num\n",
      "tk_punto_y_coma\n",
      "tk_punto\n",
      "tk_mayorque\n",
      "tk_menorque\n",
      "tk_menos\n",
      "tk_asig\n",
      "tk_igual\n",
      "tk_separa\n",
      "tk_coma\n",
      "tk_par_izq\n",
      "tk_div\n",
      "tk_dos_puntos\n"
     ]
    }
   ],
   "source": [
    "for tk in tks:\n",
    "    print(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writes\n",
      "int\n",
      "returns\n",
      "procedure\n",
      "resource\n",
      "mod\n",
      "to\n",
      "body\n",
      "send\n",
      "fa\n",
      "or\n",
      "end\n",
      "read\n",
      "if\n",
      "cap\n",
      "write\n",
      "fi\n",
      "getarg\n",
      "var\n",
      "global\n",
      "stop\n",
      "create\n",
      "id\n",
      "af\n",
      "import\n"
     ]
    }
   ],
   "source": [
    "for i in ids:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
