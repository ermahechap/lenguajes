{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Lexer.Lexer as lx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_depth = 500\n",
    "max_depth = default_depth\n",
    "\n",
    "class SyntaxBuilder:\n",
    "    def __init__(self,path_grammar, init_symbol = 'S'):\n",
    "        self.path_grammar = path_grammar\n",
    "        \n",
    "        self.init_symbol = init_symbol\n",
    "        self.grammar = {}\n",
    "        self.non_terminals = set()\n",
    "        \n",
    "        self.first = {}\n",
    "        \n",
    "        self.following = {}\n",
    "        self.explored = set() #set to keep state of following\n",
    "        \n",
    "        self.predictions = {}\n",
    "        self.getProd = []\n",
    "        self.getId = {}\n",
    "        \n",
    "    def loadGrammar(self):\n",
    "        f = open(self.path_grammar)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        id_ = 0\n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in self.grammar:\n",
    "                self.non_terminals.add(line[0])\n",
    "                self.first[line[0]] = set()\n",
    "                self.following[line[0]] = set()\n",
    "                self.predictions[line[0]] = {}\n",
    "                self.grammar[line[0]] = []\n",
    "            \n",
    "            self.getProd.append(line[1:])\n",
    "            self.getId[str(line[1:])] = id_\n",
    "            self.predictions[line[0]][id_] = set()\n",
    "            self.grammar[line[0]].append(line[1:])\n",
    "            id_+=1\n",
    "        self.following[self.init_symbol] = {'$'} # Add to first symbol\n",
    "        \n",
    "    def primeros(self, v, precalc = False):\n",
    "        global max_depth\n",
    "        max_depth-=1\n",
    "        \n",
    "        if max_depth <=0 or len(v)==0:\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if len(v) == 1 and v[0]=='e':\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if v[0] not in self.non_terminals:\n",
    "            max_depth+=1\n",
    "            return {v[0]}\n",
    "        \n",
    "        if len(v) == 1 and v[0] in self.non_terminals:\n",
    "            if precalc: return self.first[v[0]] # Used when we have already calculated it for non-terminals\n",
    "            \n",
    "            productions = self.grammar[v[0]] \n",
    "            first = set()\n",
    "            for p in productions:\n",
    "                first |= self.primeros(p)\n",
    "            max_depth+=1\n",
    "            self.first[v[0]] |= first\n",
    "            return first\n",
    "        \n",
    "        first = self.primeros([v[0]])\n",
    "        \n",
    "        if 'e' in first:\n",
    "            if len(v)>1:\n",
    "                first.discard('e')\n",
    "                first |= self.primeros(v[1:])\n",
    "        max_depth+=1\n",
    "        return first\n",
    "    \n",
    "    \n",
    "    def siguiente(self, non_terminal): # S is the non-terminal\n",
    "        global max_depth, default_depth\n",
    "        \n",
    "        self.explored.add(non_terminal)\n",
    "        \n",
    "        for production in self.grammar[non_terminal]:\n",
    "            for i in range(len(production)):\n",
    "                p = production[i]\n",
    "                if p in self.non_terminals:\n",
    "                    if p not in self.explored:\n",
    "                        self.siguiente(p)\n",
    "                    \n",
    "                    max_depth = default_depth\n",
    "                    first = self.primeros(production[i+1:])\n",
    "                    \n",
    "                    self.following[p] |= first - {'e'}\n",
    "                    if 'e' in first:\n",
    "                        self.following[p].add(non_terminal)\n",
    "    def predict(self, S, prod):\n",
    "        first = self.primeros(prod, True)\n",
    "        if 'e' in first:\n",
    "            first.discard('e')\n",
    "            return first | self.following[S]\n",
    "        else:\n",
    "            return first\n",
    "        \n",
    "    def calcFirsts(self):\n",
    "        global max_depth, default_depth\n",
    "        for S in self.non_terminals:\n",
    "            max_depth = default_depth\n",
    "            self.primeros([S])\n",
    "    \n",
    "    def calcFollowing(self):\n",
    "        \n",
    "        for non_terminal in self.non_terminals:\n",
    "            if non_terminal not in self.explored:\n",
    "                self.siguiente(self.init_symbol)\n",
    "        \n",
    "        added = True #Placeholder, does nothing\n",
    "        while added:\n",
    "            added = False\n",
    "            for non_terminal in self.non_terminals:\n",
    "                current = self.following[non_terminal].copy()\n",
    "                for element in self.following[non_terminal]:\n",
    "                    if element in self.non_terminals:\n",
    "                        to_add = self.following[element]\n",
    "                        added = True\n",
    "                        current |= to_add\n",
    "                        current -= {non_terminal}\n",
    "                        current -= {element}\n",
    "                self.following[non_terminal] = current\n",
    "    \n",
    "    def calcPredictions(self):\n",
    "        for k,productions in self.grammar.items():\n",
    "            for production in productions:\n",
    "                self.predictions[k][self.getId[str(production)]] = self.predict(k,production)\n",
    "    \n",
    "    def calculateAll(self):\n",
    "        self.calcFirsts()\n",
    "        self.calcFollowing()\n",
    "        self.calcPredictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {0: {'cow', 'bus', 'cat', 'big'}, 1: {'ant'}}, 'B': {2: {'big'}, 3: {'bus'}, 4: {'cow', 'cat'}}, 'C': {5: {'cat'}, 6: {'cow'}}}\n",
      "[['B', 'C'], ['ant', 'A', 'all'], ['big', 'C'], ['bus', 'A', 'boss'], ['e'], ['cat'], ['cow']]\n"
     ]
    }
   ],
   "source": [
    "grammar_path = 'grammar.txt'\n",
    "file_path = 'input.txt'\n",
    "token_path = 'Lexer/tokens.txt'\n",
    "reserved_path = 'Lexer/reserved.txt'\n",
    "\n",
    "# initialize grammar and grammar sets\n",
    "grammar = SyntaxBuilder(grammar_path,'A')\n",
    "grammar.loadGrammar()\n",
    "grammar.calculateAll()\n",
    "print(grammar.predictions)\n",
    "print(grammar.getProd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivation = [\"A\", \"$\"]\n",
    "lexer = lx.Lexer(file_path)\n",
    "token_lexeme = {}\n",
    "\n",
    "# ------ UTIL ----------\n",
    "\n",
    "def get_lexeme(type_):\n",
    "    global token_lexeme\n",
    "    \n",
    "    if type_ in token_lexeme: return token_lexeme[type_] #if token is tk_???\n",
    "    return type_ # if token is reserved word\n",
    "\n",
    "def loadTkSymb(token_path):\n",
    "    global token_to_symb\n",
    "    f = open(token_path)\n",
    "    token_array = [x.strip().split('\\t') for x in f.readlines()]\n",
    "    f.close\n",
    "    token_lexeme = {k:v for v,k in token_array}\n",
    "\n",
    "# --------- MAIN ------------\n",
    "\n",
    "def mainExists(file_path):\n",
    "    # Here we find main on file\n",
    "    lexer = lx.Lexer(file_path)\n",
    "    lexer.readFile()\n",
    "    tk = lexer.nextToken()\n",
    "    while (tk.lexeme != '$'):\n",
    "        if tk.lexeme == 'resource': return True\n",
    "        tk = lexer.nextTokent()\n",
    "    return False\n",
    "\n",
    "def getNewPrefix(non_terminal, token_type):\n",
    "    global grammar\n",
    "    predictions = grammar.predictions[non_terminal]\n",
    "    \n",
    "    allTk = set()\n",
    "    for i, prediction in predictions:\n",
    "        allTk |= prediction\n",
    "        if token_type in prediction:\n",
    "            return grammar.getProd[i]\n",
    "    \n",
    "    return list(allTk) # In case we cannot solve the prefix\n",
    "        \n",
    "\n",
    "def derivate():\n",
    "    global derivation, lexer, grammar\n",
    "    lexer = lx.Lexer(file_path)\n",
    "    lexer.readFile()\n",
    "    tk = lexer.nextToken()\n",
    "    prefix = []\n",
    "    while(len(derivation)):\n",
    "        a = derivation[0]\n",
    "        \n",
    "        if a in grammar.non_terminals: # Expand\n",
    "            new_prefix = getPrediction(a ,tk.token_type)\n",
    "            derivation = new_prefix + derivation[1:]\n",
    "            prefix = new_prefix\n",
    "        \n",
    "        elif a == tk.token_type: # Match\n",
    "            tk = lexer.nextToken()\n",
    "            derivation = derivation[1:]\n",
    "        else:\n",
    "            return tk, prefix # It means we have unsatisfied expected values\n",
    "    return tk, [] # it means it finished correctly\n",
    "\n",
    "def execute():\n",
    "    global derivation, lexer, file_path\n",
    "    derivation = ['S','$']\n",
    "    \n",
    "    tk, answer = derivate()\n",
    "    \n",
    "    if len(answer): # We have not found a proper derivation\n",
    "        answer = str(answer).strip('[]')\n",
    "        print('<{},{}> Error sintactico: se encontro>: \"{}\"; se esperaba: {}.'.format(tk.row, tk.col, get_lexeme(tk.lexeme), answer))\n",
    "        return\n",
    "    \n",
    "    # we finished the code processing\n",
    "    print('El analisis sintactico ha finalizado exitosamente.')\n",
    "\n",
    "\n",
    "def main(grammar, derivation):\n",
    "    global file_path, lexer\n",
    "    loadTkSymb()\n",
    "    if not mainExists(file_path):\n",
    "        print('Error sintactico: falta funcion_principal')\n",
    "        return\n",
    "    \n",
    "    execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tk_par_izq': '(', 'tk_par_der': ')', 'tk_increment': '++', 'tk_decrement': '--', 'tk_puntero': '^', 'tk_bit-wise': '~', 'tk_mas': '+', 'tk_menos': '-', 'tk_direccion': '@', 'tk_num_llamadas': '?', 'tk_exp': '**', 'tk_multi': '*', 'tk_div': '/', 'tk_residuo': '%', 'tk_swap': ':=:', 'tk_conc': '||', 'tk_left_shift': '<<', 'tk_right_shift': '>>', 'tk_igual': '=', 'tk_coma': ',', 'tk_punto_y_coma': ';', 'tk_asig': ':=', 'tk_dos_puntos': ':', 'tk_punto': '.', 'tk_ejecuta': '->', 'tk_inc_asign': '+:=', 'tk_dec_asign': '-:=', 'tk_mult_asign': '*:=', 'tk_div_asign': '/:=', 'tk_rem_asign': '%:=', 'tk_exp_asign': '**:=', 'tk_or_asign': '|:=', 'tk_and_asign': '&:=', 'tk_concat_asign': '||:=', 'tk_left_shift_asign': '<<:=', 'tk_right_shift_asign': '>>:=', 'tk_distinto': '~=', 'tk_cor_izq': '[', 'tk_cor_der': ']', 'tk_menorque': '<', 'tk_mayorque': '>', 'tk_mayor_igual': '>=', 'tk_menor_igual': '<=', 'tk_separa': '[]', 'tk_llave_izq': '{', 'tk_llave_der': '}', 'tk_slice': '...', 'tk_div_sum': '/+', 'tk_mod': 'mod', 'tk_and': '&', 'tk_or': '|', 'tk_xor': 'xor'}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = list({1,2,3,4})\n",
    "str(b).strip('[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
