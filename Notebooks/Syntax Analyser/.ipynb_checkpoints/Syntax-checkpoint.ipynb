{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Lexer.Lexer as lx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_depth = 600\n",
    "max_depth = default_depth\n",
    "\n",
    "class SyntaxBuilder:\n",
    "    def __init__(self,path_grammar, init_symbol = 'S'):\n",
    "        self.path_grammar = path_grammar\n",
    "        \n",
    "        self.init_symbol = init_symbol\n",
    "        self.grammar = {}\n",
    "        self.non_terminals = set()\n",
    "        \n",
    "        self.first = {}\n",
    "        \n",
    "        self.following = {}\n",
    "        self.explored = set() #set to keep state of following\n",
    "        \n",
    "        self.predictions = {}\n",
    "        self.getProd = []\n",
    "        self.getId = {}\n",
    "        \n",
    "    def loadGrammar(self):\n",
    "        f = open(self.path_grammar)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        id_ = 0\n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            if line == [] or line[0]=='//': continue\n",
    "            if line[0] not in self.grammar:\n",
    "                self.non_terminals.add(line[0])\n",
    "                self.first[line[0]] = set()\n",
    "                self.following[line[0]] = set()\n",
    "                self.predictions[line[0]] = {}\n",
    "                self.grammar[line[0]] = []\n",
    "            \n",
    "            self.getProd.append(line[1:])\n",
    "            self.getId[str(line[1:])] = id_\n",
    "            self.predictions[line[0]][id_] = set()\n",
    "            self.grammar[line[0]].append(line[1:])\n",
    "            id_+=1\n",
    "        self.following[self.init_symbol] = {'$'} # Add to first symbol\n",
    "        \n",
    "    def primeros(self, v, precalc = False):\n",
    "        print(v)\n",
    "        global max_depth\n",
    "        max_depth-=1\n",
    "        \n",
    "        if max_depth <=0 or len(v)==0:\n",
    "            #print('MAX_DEPTH REACHED')\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if len(v) == 1 and v[0]=='e':\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if v[0] not in self.non_terminals:\n",
    "            max_depth+=1\n",
    "            return {v[0]}\n",
    "        \n",
    "        if len(v) == 1 and v[0] in self.non_terminals:\n",
    "            #print('<<<<<',v[0])\n",
    "            if precalc: return self.first[v[0]] # Used when we have already calculated it for non-terminals\n",
    "            \n",
    "            productions = self.grammar[v[0]] \n",
    "            first = set()\n",
    "            for p in productions:\n",
    "                first |= self.primeros(p)\n",
    "            max_depth+=1\n",
    "            self.first[v[0]] |= first\n",
    "            return first\n",
    "        \n",
    "        first = self.primeros([v[0]])\n",
    "        \n",
    "        if 'e' in first:\n",
    "            if len(v)>1:\n",
    "                first.discard('e')\n",
    "                first |= self.primeros(v[1:])\n",
    "        max_depth+=1\n",
    "        return first\n",
    "    \n",
    "    \n",
    "    def siguiente(self, non_terminal): # S is the non-terminal\n",
    "        global max_depth, default_depth\n",
    "        \n",
    "        self.explored.add(non_terminal)\n",
    "        \n",
    "        for production in self.grammar[non_terminal]:\n",
    "            for i in range(len(production)):\n",
    "                p = production[i]\n",
    "                if p in self.non_terminals:\n",
    "                    if p not in self.explored:\n",
    "                        self.siguiente(p)\n",
    "                    \n",
    "                    max_depth = default_depth\n",
    "                    first = self.primeros(production[i+1:])\n",
    "                    \n",
    "                    self.following[p] |= first - {'e'}\n",
    "                    if 'e' in first:\n",
    "                        self.following[p].add(non_terminal)\n",
    "    def predict(self, S, prod):\n",
    "        first = self.primeros(prod, False)\n",
    "        if 'e' in first:\n",
    "            first.discard('e')\n",
    "            return first | self.following[S]\n",
    "        else:\n",
    "            return first\n",
    "        \n",
    "    def calcFirsts(self):\n",
    "        global max_depth, default_depth\n",
    "        for S in self.grammar:\n",
    "            max_depth = 600\n",
    "            print(\"PROCESSING ----\",S)\n",
    "            self.primeros([S])\n",
    "    \n",
    "    def calcFollowing(self):\n",
    "        \n",
    "        for non_terminal in self.non_terminals:\n",
    "            if non_terminal not in self.explored:\n",
    "                self.siguiente(self.init_symbol)\n",
    "        \n",
    "        added = True #Placeholder, does nothing\n",
    "        while added:\n",
    "            added = False\n",
    "            for non_terminal in self.non_terminals:\n",
    "                current = self.following[non_terminal].copy()\n",
    "                for element in self.following[non_terminal]:\n",
    "                    if element in self.non_terminals:\n",
    "                        to_add = self.following[element]\n",
    "                        added = True\n",
    "                        current |= to_add\n",
    "                        current -= {non_terminal}\n",
    "                        current -= {element}\n",
    "                self.following[non_terminal] = current\n",
    "    \n",
    "    def calcPredictions(self):\n",
    "        for k,productions in self.grammar.items():\n",
    "            for production in productions:\n",
    "                self.predictions[k][self.getId[str(production)]] = self.predict(k,production)\n",
    "    \n",
    "    def calculateAll(self):\n",
    "        print(\"<<<INIT FIRST\")\n",
    "        self.calcFirsts()\n",
    "        print(\">>>FIRST DONE\")\n",
    "        print(\"<<<INIT FOLLOWING\")\n",
    "        self.calcFollowing()\n",
    "        print(\">>>FOLLOWING DONE\")\n",
    "        print(\"<<<INIT PREDICTIONS\")\n",
    "        self.calcPredictions()\n",
    "        print(\">>>PREDICTIONS DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRY GLOBAL_BODY\n",
      "MAIN TYPE_DEF\n",
      "RESOURCE OTHER_ELEMENT_ONLY\n",
      "GLOBAL RETURNS\n",
      "GLOBAL_BODY OP_DEF\n",
      "SPECIFICATION IMPORT\n",
      "SPECIFICATION_BODY SLICE_FROM\n",
      "RESOURCE_BODY TYPE\n",
      "TYPE IDENTIFIER\n",
      "OP_TYPE_DEF OTHER_ELEMENT\n",
      "OP_DEF SPECIFICATION_BODY\n",
      "OTHER_ELEMENT OP_PARAMS\n",
      "DECLARATION_TYPE OP_TYPE_DEF\n",
      "TYPE_DEF RETUNRN_TYPE\n",
      "OP_DEF_ONLY IMPORT_LIST\n",
      "OTHER_ELEMENT_ONLY CONSTANT\n",
      "RETURNS SLICE\n",
      "RETUNRN_TYPE IMPORT_LIST_OTHER\n",
      "IMPORT PARAMS\n",
      "IMPORT_LIST SLICE_TO\n",
      "IMPORT_LIST_OTHER OP_PARAMS_OTHER\n",
      "OPERATION RESOURCE\n",
      "OP_PARAMS ARITHM\n",
      "OP_PARAMS_OTHER OPERATION\n",
      "CONSTANT DECLARATION_TYPE\n",
      "VAR GLOBAL\n",
      "PARAMS IDENTIFIER_PREFIX\n",
      "DATA_TYPE_ASSIGN DATA_TYPE\n",
      "DATA_TYPE VAR\n",
      "SLICE DATA_TYPE_ASSIGN\n",
      "SLICE_FROM MAIN\n",
      "SLICE_TO SPECIFICATION\n",
      "IDENTIFIER ENTRY\n",
      "IDENTIFIER_PREFIX OP_DEF_ONLY\n",
      "ARITHM RESOURCE_BODY\n"
     ]
    }
   ],
   "source": [
    "grammar_path = 'grammar2.txt'\n",
    "file_path = 'input.txt'\n",
    "token_path = 'Lexer/tokens.txt'\n",
    "reserved_path = 'Lexer/reserved.txt'\n",
    "# initialize grammar and grammar sets\n",
    "grammar = SyntaxBuilder(grammar_path,'ENTRY')\n",
    "grammar.loadGrammar()\n",
    "for k,S in zip(grammar.grammar.keys(), grammar.non_terminals):\n",
    "    print(k,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in grammar.grammar:\n",
    "    max_depth = default_depth\n",
    "    grammar.primeros([k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<INIT FIRST\n",
      "PROCESSING ---- OP_DEF_ONLY\n",
      "PROCESSING ---- TYPE_DEF\n",
      "PROCESSING ---- DATA_TYPE_ASSIGN\n",
      "PROCESSING ---- SLICE_FROM\n",
      "PROCESSING ---- CONSTANT\n",
      "PROCESSING ---- TYPE\n",
      "PROCESSING ---- RETUNRN_TYPE\n",
      "PROCESSING ---- OPERATION\n",
      "PROCESSING ---- IMPORT_LIST\n",
      "PROCESSING ---- GLOBAL_BODY\n",
      "PROCESSING ---- DECLARATION_TYPE\n",
      "PROCESSING ---- RESOURCE\n",
      "PROCESSING ---- IMPORT_LIST_OTHER\n",
      "PROCESSING ---- OP_PARAMS\n",
      "PROCESSING ---- VAR\n",
      "PROCESSING ---- DATA_TYPE\n",
      "PROCESSING ---- OP_DEF\n",
      "PROCESSING ---- IMPORT\n",
      "PROCESSING ---- SPECIFICATION_BODY\n",
      "PROCESSING ---- GLOBAL\n",
      "PROCESSING ---- RESOURCE_BODY\n",
      "PROCESSING ---- SLICE_TO\n",
      "PROCESSING ---- OTHER_ELEMENT_ONLY\n",
      "PROCESSING ---- MAIN\n"
     ]
    }
   ],
   "source": [
    "grammar.loadGrammar()\n",
    "grammar.calculateAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grammar.predictions)\n",
    "print(grammar.getProd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivation = [\"ENTRY\", \"$\"]\n",
    "lexer = lx.Lexer(file_path)\n",
    "token_lexeme = {}\n",
    "\n",
    "# ------ UTIL ----------\n",
    "\n",
    "def get_lexeme(type_):\n",
    "    global token_lexeme\n",
    "    \n",
    "    if type_ in token_lexeme: return token_lexeme[type_] #if token is tk_???\n",
    "    return type_ # if token is reserved word\n",
    "\n",
    "def loadTkSymb():\n",
    "    global token_to_symb,token_path\n",
    "    f = open(token_path)\n",
    "    token_array = [x.strip().split('\\t') for x in f.readlines()]\n",
    "    f.close\n",
    "    token_lexeme = {k:v for v,k in token_array}\n",
    "\n",
    "# --------- MAIN ------------\n",
    "\n",
    "def mainExists(file_path):\n",
    "    # Here we find main on file\n",
    "    lexer = lx.Lexer(file_path)\n",
    "    lexer.readFile()\n",
    "    tk = lexer.nextToken()\n",
    "    while (tk.lexeme != '$'):\n",
    "        if tk.lexeme == 'resource': return True\n",
    "        tk = lexer.nextTokent()\n",
    "    return False\n",
    "\n",
    "def getNewPrefix(non_terminal, token_type):\n",
    "    global grammar\n",
    "    predictions = grammar.predictions[non_terminal]\n",
    "    print(\"<<<PREDICTIONS: \")\n",
    "    allTk = set()\n",
    "    for i, prediction in predictions.items():\n",
    "        allTk |= prediction\n",
    "        print(\"<<<<<<<<<<<<<\",grammar.getProd[i],prediction)\n",
    "        if token_type in prediction:\n",
    "            return grammar.getProd[i]\n",
    "    \n",
    "    return list(allTk) # In case we cannot solve the prefix\n",
    "        \n",
    "\n",
    "def derivate():\n",
    "    global derivation, lexer, grammar\n",
    "    lexer = lx.Lexer(file_path)\n",
    "    lexer.readFile()\n",
    "    tk = lexer.nextToken()\n",
    "    prefix = []\n",
    "    while(len(derivation)):\n",
    "        print(\"-------------\")\n",
    "        a = derivation[0]\n",
    "        print(\">>>>\",derivation)\n",
    "        print(\"<<<TK: \",tk.parse())\n",
    "        if a in grammar.non_terminals: # Expand\n",
    "            new_prefix = getNewPrefix(a ,tk.token_type)\n",
    "            derivation = new_prefix + derivation[1:]\n",
    "            prefix = new_prefix\n",
    "            print('')\n",
    "        \n",
    "        elif a == tk.token_type: # Match\n",
    "            tk = lexer.nextToken()\n",
    "            derivation = derivation[1:]\n",
    "        else:\n",
    "            print(\"-----\",derivation)\n",
    "            print(\"-----TK: \",tk.parse())\n",
    "            return tk, prefix # It means we have unsatisfied expected values\n",
    "    return tk, [] # it means it finished correctly\n",
    "\n",
    "def execute():\n",
    "    global derivation\n",
    "    derivation = ['COMPONENT','$']\n",
    "    \n",
    "    tk, answer = derivate()\n",
    "    \n",
    "    if len(answer): # We have not found a proper derivation\n",
    "        answer = str(answer).strip('[]')\n",
    "        print('<{},{}> Error sintactico: se encontro>: \"{}\"; se esperaba: {}.'.format(tk.row, tk.col, get_lexeme(tk.lexeme), answer))\n",
    "        return\n",
    "    \n",
    "    # we finished the code processing\n",
    "    print('El analisis sintactico ha finalizado exitosamente.')\n",
    "\n",
    "\n",
    "def main():\n",
    "    global file_path, lexer\n",
    "    loadTkSymb()\n",
    "    if not mainExists(file_path):\n",
    "        print('Error sintactico: falta funcion_principal')\n",
    "        return\n",
    "    \n",
    "    execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.non_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list({1,2,3,4})\n",
    "str(b).strip('[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
