{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Lexer.Lexer as lx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_depth = 600\n",
    "max_depth = default_depth\n",
    "\n",
    "class SyntaxBuilder:\n",
    "    def __init__(self,path_grammar,path_input, init_symbol = 'S'):\n",
    "        self.path_grammar = path_grammar\n",
    "        self.path_input = path_input\n",
    "        \n",
    "        self.init_symbol = init_symbol\n",
    "        self.grammar = {}\n",
    "        self.non_terminals = set()\n",
    "        \n",
    "        self.first = {}\n",
    "        \n",
    "        self.following = {}\n",
    "        self.explored = set() #set to keep state of following\n",
    "        \n",
    "        self.predictions = {}\n",
    "        self.getProd = []\n",
    "        self.getId = {}\n",
    "        \n",
    "    def loadGrammar(self):\n",
    "        f = open(self.path_grammar)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        id_ = 0\n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            if line == [] or line[0]=='//': continue\n",
    "            if line[0] not in self.grammar:\n",
    "                self.non_terminals.add(line[0])\n",
    "                self.first[line[0]] = set()\n",
    "                self.following[line[0]] = set()\n",
    "                self.predictions[line[0]] = {}\n",
    "                self.grammar[line[0]] = []\n",
    "            \n",
    "            self.getProd.append(line[1:])\n",
    "            self.getId[str(line[1:])] = id_\n",
    "            self.predictions[line[0]][id_] = set()\n",
    "            self.grammar[line[0]].append(line[1:])\n",
    "            id_+=1\n",
    "        self.following[self.init_symbol] = {'$'} # Add to first symbol\n",
    "        \n",
    "    def primeros(self, v, precalc = False):\n",
    "        global max_depth\n",
    "        max_depth-=1\n",
    "        \n",
    "        if max_depth <=0 or len(v)==0:\n",
    "            #print('MAX_DEPTH REACHED')\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if len(v) == 1 and v[0]=='e':\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if v[0] not in self.non_terminals:\n",
    "            max_depth+=1\n",
    "            return {v[0]}\n",
    "        \n",
    "        if len(v) == 1 and v[0] in self.non_terminals:\n",
    "            #print('<<<<<',v[0])\n",
    "            if precalc: return self.first[v[0]] # Used when we have already calculated it for non-terminals\n",
    "            \n",
    "            productions = self.grammar[v[0]] \n",
    "            first = set()\n",
    "            for p in productions:\n",
    "                first |= self.primeros(p)\n",
    "            max_depth+=1\n",
    "            self.first[v[0]] |= first\n",
    "            return first\n",
    "        \n",
    "        first = self.primeros([v[0]])\n",
    "        \n",
    "        if 'e' in first:\n",
    "            if len(v)>1:\n",
    "                first.discard('e')\n",
    "                first |= self.primeros(v[1:])\n",
    "        max_depth+=1\n",
    "        return first\n",
    "    \n",
    "    \n",
    "    def siguiente(self, non_terminal): # S is the non-terminal\n",
    "        global max_depth, default_depth\n",
    "        \n",
    "        self.explored.add(non_terminal)\n",
    "        \n",
    "        for production in self.grammar[non_terminal]:\n",
    "            for i in range(len(production)):\n",
    "                p = production[i]\n",
    "                if p in self.non_terminals:\n",
    "                    if p not in self.explored:\n",
    "                        self.siguiente(p)\n",
    "                    \n",
    "                    max_depth = default_depth\n",
    "                    first = self.primeros(production[i+1:])\n",
    "                    \n",
    "                    self.following[p] |= first - {'e'}\n",
    "                    if 'e' in first:\n",
    "                        self.following[p].add(non_terminal)\n",
    "    def predict(self, S, prod):\n",
    "        first = self.primeros(prod, False)\n",
    "        if 'e' in first:\n",
    "            first.discard('e')\n",
    "            return first | self.following[S]\n",
    "        else:\n",
    "            return first\n",
    "        \n",
    "    def calcFirsts(self):\n",
    "        global max_depth, default_depth\n",
    "        for S in self.grammar:\n",
    "            max_depth = 600\n",
    "            self.primeros([S])\n",
    "    \n",
    "    def calcFollowing(self):\n",
    "        \n",
    "        for non_terminal in self.non_terminals:\n",
    "            if non_terminal not in self.explored:\n",
    "                self.siguiente(self.init_symbol)\n",
    "        \n",
    "        added = True #Placeholder, does nothing\n",
    "        while added:\n",
    "            added = False\n",
    "            for non_terminal in self.non_terminals:\n",
    "                current = self.following[non_terminal].copy()\n",
    "                for element in self.following[non_terminal]:\n",
    "                    if element in self.non_terminals:\n",
    "                        to_add = self.following[element]\n",
    "                        added = True\n",
    "                        current |= to_add\n",
    "                        current -= {non_terminal}\n",
    "                        current -= {element}\n",
    "                self.following[non_terminal] = current\n",
    "    \n",
    "    def calcPredictions(self):\n",
    "        for k,productions in self.grammar.items():\n",
    "            for production in productions:\n",
    "                self.predictions[k][self.getId[str(production)]] = self.predict(k,production)\n",
    "    \n",
    "    def calculateAll(self):\n",
    "        print(\"<<<INIT FIRST\")\n",
    "        self.calcFirsts()\n",
    "        print(\">>>FIRST DONE\")\n",
    "        print(\"<<<INIT FOLLOWING\")\n",
    "        self.calcFollowing()\n",
    "        print(\">>>FOLLOWING DONE\")\n",
    "        print(\"<<<INIT PREDICTIONS\")\n",
    "        self.calcPredictions()\n",
    "        print(\">>>PREDICTIONS DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_path = 'grammar.txt'\n",
    "file_path = 'input.txt'\n",
    "token_path = 'Lexer/tokens.txt'\n",
    "reserved_path = 'Lexer/reserved.txt'\n",
    "# initialize grammar and grammar sets\n",
    "grammar = SyntaxBuilder(grammar_path,'S')\n",
    "grammar.loadGrammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<INIT FIRST\n",
      ">>>FIRST DONE\n",
      "<<<INIT FOLLOWING\n",
      ">>>FOLLOWING DONE\n",
      "<<<INIT PREDICTIONS\n",
      ">>>PREDICTIONS DONE\n"
     ]
    }
   ],
   "source": [
    "grammar.calculateAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = lexer.nextToken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'token_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-d8b4ce8be9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'token_type'"
     ]
    }
   ],
   "source": [
    "token.token_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "['A', 'B', 'C', \"S'\"] {'dos', 'uno', 'cuatro', '$', 'tres'}\n",
      "S'\n",
      "['uno', \"S'\"] {'uno'}\n",
      "['e'] set()\n",
      "['e'] {'$'}\n",
      "A\n",
      "['dos', 'B', 'C'] {'dos'}\n",
      "['e'] set()\n",
      "['e'] {'$', 'uno', 'tres', 'cuatro'}\n",
      "B\n",
      "['C', 'tres'] {'tres', 'cuatro'}\n",
      "['e'] set()\n",
      "['e'] {'$', 'uno', 'tres', 'cuatro'}\n",
      "C\n",
      "['cuatro', 'B'] {'cuatro'}\n",
      "['e'] {'$', 'uno', 'tres', 'cuatro'}\n"
     ]
    }
   ],
   "source": [
    "for k,v in grammar.predictions.items():\n",
    "    print(k)\n",
    "    for p,q in v.items():\n",
    "        print(grammar.getProd[p],q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grammar.predictions)\n",
    "print(grammar.getProd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivation = [\"ENTRY\", \"$\"]\n",
    "lexer = lx.Lexer(file_path)\n",
    "token_lexeme = {}\n",
    "\n",
    "# ------ UTIL ----------\n",
    "\n",
    "def get_lexeme(type_):\n",
    "    global token_lexeme\n",
    "    \n",
    "    if type_ in token_lexeme: return token_lexeme[type_] #if token is tk_???\n",
    "    return type_ # if token is reserved word\n",
    "\n",
    "def loadTkSymb():\n",
    "    global token_to_symb,token_path\n",
    "    f = open(token_path)\n",
    "    token_array = [x.strip().split('\\t') for x in f.readlines()]\n",
    "    f.close\n",
    "    token_lexeme = {k:v for v,k in token_array}\n",
    "\n",
    "# --------- MAIN ------------\n",
    "\n",
    "def mainExists(file_path):\n",
    "    # Here we find main on file\n",
    "    lexer = lx.Lexer(file_path)\n",
    "    lexer.readFile()\n",
    "    tk = lexer.nextToken()\n",
    "    while (tk.lexeme != '$'):\n",
    "        if tk.lexeme == 'resource': return True\n",
    "        tk = lexer.nextTokent()\n",
    "    return False\n",
    "\n",
    "def getNewPrefix(non_terminal, token_type):\n",
    "    global grammar\n",
    "    predictions = grammar.predictions[non_terminal]\n",
    "    print(\"<<<PREDICTIONS: \")\n",
    "    allTk = set()\n",
    "    for i, prediction in predictions.items():\n",
    "        allTk |= prediction\n",
    "        print(\"<<<<<<<<<<<<<\",grammar.getProd[i],prediction)\n",
    "        if token_type in prediction:\n",
    "            return grammar.getProd[i]\n",
    "    \n",
    "    return list(allTk) # In case we cannot solve the prefix\n",
    "        \n",
    "\n",
    "def derivate():\n",
    "    global derivation, lexer, grammar\n",
    "    lexer = lx.Lexer(file_path)\n",
    "    lexer.readFile()\n",
    "    tk = lexer.nextToken()\n",
    "    prefix = []\n",
    "    while(len(derivation)):\n",
    "        print(\"-------------\")\n",
    "        a = derivation[0]\n",
    "        print(\">>>>\",derivation)\n",
    "        print(\"<<<TK: \",tk.parse())\n",
    "        if a in grammar.non_terminals: # Expand\n",
    "            new_prefix = getNewPrefix(a ,tk.token_type)\n",
    "            derivation = new_prefix + derivation[1:]\n",
    "            prefix = new_prefix\n",
    "            print('')\n",
    "        \n",
    "        elif a == tk.token_type: # Match\n",
    "            tk = lexer.nextToken()\n",
    "            derivation = derivation[1:]\n",
    "        else:\n",
    "            print(\"-----\",derivation)\n",
    "            print(\"-----TK: \",tk.parse())\n",
    "            return tk, prefix # It means we have unsatisfied expected values\n",
    "    return tk, [] # it means it finished correctly\n",
    "\n",
    "def execute():\n",
    "    global derivation\n",
    "    derivation = ['COMPONENT','$']\n",
    "    \n",
    "    tk, answer = derivate()\n",
    "    \n",
    "    if len(answer): # We have not found a proper derivation\n",
    "        answer = str(answer).strip('[]')\n",
    "        print('<{},{}> Error sintactico: se encontro>: \"{}\"; se esperaba: {}.'.format(tk.row, tk.col, get_lexeme(tk.lexeme), answer))\n",
    "        return\n",
    "    \n",
    "    # we finished the code processing\n",
    "    print('El analisis sintactico ha finalizado exitosamente.')\n",
    "\n",
    "\n",
    "def main():\n",
    "    global file_path, lexer\n",
    "    loadTkSymb()\n",
    "    if not mainExists(file_path):\n",
    "        print('Error sintactico: falta funcion_principal')\n",
    "        return\n",
    "    \n",
    "    execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.non_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list({1,2,3,4})\n",
    "str(b).strip('[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<tk_asig,4,13>'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
