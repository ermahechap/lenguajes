{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Lexer.Lexer as lx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_grammar='grammar.txt'\n",
    "path_tree = 'tree.txt'\n",
    "path_input='input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_depth = 500\n",
    "max_depth = default_depth\n",
    "\n",
    "class SyntaxBuilder:\n",
    "    def __init__(self,path_grammar,path_input, init_symbol = 'S'):\n",
    "        self.path_grammar = path_grammar\n",
    "        self.path_input = path_input\n",
    "        \n",
    "        self.init_symbol = init_symbol\n",
    "        self.grammar = {}\n",
    "        self.non_terminals = set()\n",
    "        \n",
    "        self.first = {}\n",
    "        \n",
    "        self.following = {}\n",
    "        self.explored = set() #set to keep state of following\n",
    "        \n",
    "        self.predictions = {}\n",
    "        \n",
    "    def loadGrammar(self):\n",
    "        f = open(self.path_grammar)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in self.grammar:\n",
    "                self.non_terminals.add(line[0])\n",
    "                self.first[line[0]] = set()\n",
    "                self.following[line[0]] = set()\n",
    "                self.predictions[line[0]] = []\n",
    "                self.grammar[line[0]] = []\n",
    "            self.grammar[line[0]].append(line[1:])\n",
    "        \n",
    "        self.following[self.init_symbol] = {'$'} # Add to first symbol\n",
    "        \n",
    "    def primeros(self, v, precalc = False):\n",
    "        global max_depth\n",
    "        max_depth-=1\n",
    "        \n",
    "        if max_depth <=0 or len(v)==0:\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if len(v) == 1 and v[0]=='e':\n",
    "            max_depth+=1\n",
    "            return {'e'}\n",
    "        \n",
    "        if v[0] not in self.non_terminals:\n",
    "            max_depth+=1\n",
    "            return {v[0]}\n",
    "        \n",
    "        if len(v) == 1 and v[0] in self.non_terminals:\n",
    "            if precalc: return self.first[v[0]] # Used when we have already calculated it for non-terminals\n",
    "            \n",
    "            productions = self.grammar[v[0]] \n",
    "            first = set()\n",
    "            for p in productions:\n",
    "                first |= self.primeros(p)\n",
    "            max_depth+=1\n",
    "            self.first[v[0]] |= first\n",
    "            return first\n",
    "        \n",
    "        first = self.primeros([v[0]])\n",
    "        \n",
    "        if 'e' in first:\n",
    "            if len(v)>1:\n",
    "                first.discard('e')\n",
    "                first |= self.primeros(v[1:])\n",
    "        max_depth+=1\n",
    "        return first\n",
    "    \n",
    "    \n",
    "    def siguiente(self, non_terminal): # S is the non-terminal\n",
    "        global max_depth, default_depth\n",
    "        \n",
    "        self.explored.add(non_terminal)\n",
    "        \n",
    "        for production in self.grammar[non_terminal]:\n",
    "            for i in range(len(production)):\n",
    "                p = production[i]\n",
    "                if p in self.non_terminals:\n",
    "                    if p not in self.explored:\n",
    "                        self.siguiente(p)\n",
    "                    \n",
    "                    max_depth = default_depth\n",
    "                    first = self.primeros(production[i+1:])\n",
    "                    \n",
    "                    self.following[p] |= first - {'e'}\n",
    "                    if 'e' in first:\n",
    "                        self.following[p].add(non_terminal)\n",
    "    def predict(self, S, prod):\n",
    "        first = self.primeros(prod, True)\n",
    "        if 'e' in first:\n",
    "            first.discard('e')\n",
    "            return first | self.following[S]\n",
    "        else:\n",
    "            return first\n",
    "        \n",
    "    def calcFirsts(self):\n",
    "        global max_depth, default_depth\n",
    "        for S in self.non_terminals:\n",
    "            max_depth = default_depth\n",
    "            self.primeros([S])\n",
    "    \n",
    "    def calcFollowing(self):\n",
    "        \n",
    "        for non_terminal in self.non_terminals:\n",
    "            if non_terminal not in self.explored:\n",
    "                self.siguiente(self.init_symbol)\n",
    "        \n",
    "        added = True #Placeholder, does nothing\n",
    "        while added:\n",
    "            added = False\n",
    "            for non_terminal in self.non_terminals:\n",
    "                current = self.following[non_terminal].copy()\n",
    "                for element in self.following[non_terminal]:\n",
    "                    if element in self.non_terminals:\n",
    "                        to_add = self.following[element]\n",
    "                        added = True\n",
    "                        current |= to_add\n",
    "                        current -= {non_terminal}\n",
    "                        current -= {element}\n",
    "                self.following[non_terminal] = current\n",
    "    \n",
    "    def calcPredictions(self):\n",
    "        for k,productions in self.grammar.items():\n",
    "            for production in productions:\n",
    "                self.predictions[k].append(list(self.predict(k,production)))\n",
    "    \n",
    "    def calculateAll(self):\n",
    "        self.calcFirsts()\n",
    "        self.calcFollowing()\n",
    "        self.calcPredictions()\n",
    "        \n",
    "    def predicDerivation(self):\n",
    "        self.predictions\n",
    "    \n",
    "    def derivate(self):\n",
    "        tk = \n",
    "        \n",
    "    def parse(self):\n",
    "        try:\n",
    "            tokens\n",
    "        except SyntaxEception as\n",
    "        \n",
    "    def mainExist(self):\n",
    "        lexer = lx.Lexer('input.txt')\n",
    "        lexer.readFile()\n",
    "        token = lexer.nextToken()        \n",
    "        while(token.lexeme != 'resource'):\n",
    "            print(token.lexeme)\n",
    "            token = lexer.nextToken()\n",
    "            if(token == ''):\n",
    "                return False\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sx = SyntaxBuilder(path_grammar,path_input,'S')\n",
    "    sx.loadGrammar()\n",
    "    sx.calculateAll()\n",
    "    if(not sx.mainExist()):\n",
    "        print('Error sintactico: falta funcion_pricipal')\n",
    "    else:\n",
    "        print('si esta la funcion_principal ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si esta la funcion_principal \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S': {'tk_abs'}, 'A': {'tk_parderecha'}, 'B': {'tk_int'}, 'C': {'tk_parizquierda'}}\n",
      "{'S': {'$'}, 'A': {'$'}, 'B': {'$'}, 'C': {'$'}}\n",
      "{'S': [['tk_abs']], 'A': [['tk_parderecha']], 'B': [['tk_int']], 'C': [['tk_parizquierda']]}\n"
     ]
    }
   ],
   "source": [
    "# No olvidar configurar el s[imbolo inicial\n",
    "sx = SyntaxBuilder(path_grammar,path_input,'S')\n",
    "sx.loadGrammar()\n",
    "sx.calculateAll()\n",
    "# sx.mainExist()\n",
    "lexer = lx.Lexer('input.txt')\n",
    "lexer.readFile()\n",
    "# lexer.nextToken().parse()\n",
    "\n",
    "print(sx.first)\n",
    "print(sx.following)\n",
    "print(sx.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = lexer.nextToken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'token_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-d8b4ce8be9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'token_type'"
     ]
    }
   ],
   "source": [
    "token.token_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'col',\n",
       " 'error',\n",
       " 'lexeme',\n",
       " 'parse',\n",
       " 'row',\n",
       " 'token_type']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<tk_asig,4,13>'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
